{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4831f285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Header Tags\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "#1- Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "response = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "titles = soup.find_all([\"h1\", \"h2\"])\n",
    "\n",
    "header_tags = []\n",
    "for title in titles:\n",
    "    header_tags.append(title.text)\n",
    "\n",
    "df = pd.DataFrame(header_tags, columns=['Header Tags'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fadb225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      23  2,714    118\n",
      "1     Pakistan\\nPAK      20  2,316    116\n",
      "2        India\\nIND      36  4,081    113\n",
      "3   New Zealand\\nNZ      27  2,806    104\n",
      "4      England\\nENG      24  2,426    101\n",
      "5  South Africa\\nSA      19  1,910    101\n",
      "6   Bangladesh\\nBAN      28  2,661     95\n",
      "7  Afghanistan\\nAFG      16  1,404     88\n",
      "8     Sri Lanka\\nSL      32  2,794     87\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "#3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the rankings\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "# Lists to store data\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "rating = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    team = cols[1].text.strip()\n",
    "    match = cols[2].text.strip()\n",
    "    point = cols[3].text.strip()\n",
    "    rate = cols[4].text.strip()\n",
    "    \n",
    "    teams.append(team)\n",
    "    matches.append(match)\n",
    "    points.append(point)\n",
    "    rating.append(rate)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Team': teams,'Matches': matches,'Points': points,'Rating': rating})\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2dba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team                         Rating\n",
      "0             Babar Azam  PAK  898 v West Indies, 10/06/2022\n",
      "1  Rassie van der Dussen   SA      796 v England, 19/07/2022\n",
      "2           Fakhar Zaman  PAK  784 v New Zealand, 29/04/2023\n",
      "3            Imam-ul-Haq  PAK  815 v West Indies, 12/06/2022\n",
      "4           Shubman Gill  IND  743 v West Indies, 01/08/2023\n",
      "5           Harry Tector  IRE        726 v Nepal, 04/07/2023\n",
      "6           David Warner  AUS     880 v Pakistan, 26/01/2017\n",
      "7        Quinton de Kock   SA    813 v Sri Lanka, 10/03/2019\n",
      "8            Virat Kohli  IND      911 v England, 12/07/2018\n",
      "9            Steve Smith  AUS     752 v Pakistan, 22/01/2017\n"
     ]
    }
   ],
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the ODI batsmen rankings page\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "batsmen = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    batsman = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[4].text.strip()\n",
    "    \n",
    "    batsmen.append(batsman)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Batsman': batsmen,'Team': teams,'Rating': ratings})\n",
    "\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf06684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Bowler Team                         Rating\n",
      "0    Josh Hazlewood  AUS      733 v England, 26/01/2018\n",
      "1    Mitchell Starc  AUS  783 v New Zealand, 29/03/2015\n",
      "2       Rashid Khan  AFG     806 v Pakistan, 21/09/2018\n",
      "3    Mohammed Siraj  IND  736 v New Zealand, 21/01/2023\n",
      "4        Matt Henry   NZ   691 v Bangladesh, 26/03/2021\n",
      "5  Mujeeb Ur Rahman  AFG      712 v Ireland, 24/01/2021\n",
      "6       Trent Boult   NZ    775 v Australia, 11/09/2022\n",
      "7        Adam Zampa  AUS      655 v England, 22/11/2022\n",
      "8    Shaheen Afridi  PAK  688 v West Indies, 10/06/2022\n",
      "9     Kuldeep Yadav  IND  765 v New Zealand, 26/01/2019\n"
     ]
    }
   ],
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team andrating.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "\n",
    "bowlers = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    bowler = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[4].text.strip()\n",
    "    \n",
    "    bowlers.append(bowler)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Bowler': bowlers,'Team': teams,'Rating': ratings})\n",
    "\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadce1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      26  4,290    165\n",
      "1      England\\nENG      31  3,875    125\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      30  3,039    101\n",
      "4   New Zealand\\nNZ      28  2,688     96\n",
      "5   West Indies\\nWI      29  2,743     95\n",
      "6   Bangladesh\\nBAN      17  1,284     76\n",
      "7     Sri Lanka\\nSL      12    820     68\n",
      "8     Thailand\\nTHA      13    883     68\n",
      "9     Pakistan\\nPAK      27  1,678     62\n"
     ]
    }
   ],
   "source": [
    "#4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame \n",
    "#a Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "rating = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    team = cols[1].text.strip()\n",
    "    match = cols[2].text.strip()\n",
    "    point = cols[3].text.strip()\n",
    "    rate = cols[4].text.strip()\n",
    "    \n",
    "    teams.append(team)\n",
    "    matches.append(match)\n",
    "    points.append(point)\n",
    "    rating.append(rate)\n",
    "\n",
    "df = pd.DataFrame({'Team': teams,'Matches': matches,'Points': points,'Rating': rating})\n",
    "\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e038674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team                         Rating\n",
      "0  Natalie Sciver-Brunt  ENG    803 v Australia, 18/07/2023\n",
      "1   Chamari Athapaththu   SL  758 v New Zealand, 03/07/2023\n",
      "2           Beth Mooney  AUS      776 v England, 12/07/2023\n",
      "3       Laura Wolvaardt   SA    741 v Australia, 22/03/2022\n",
      "4       Smriti Mandhana  IND      797 v England, 28/02/2019\n",
      "5          Alyssa Healy  AUS      785 v England, 03/04/2022\n",
      "6      Harmanpreet Kaur  IND      731 v England, 21/09/2022\n",
      "7          Ellyse Perry  AUS  766 v West Indies, 11/09/2019\n",
      "8           Meg Lanning  AUS  834 v New Zealand, 24/02/2016\n",
      "9       Stafanie Taylor   WI     766 v Pakistan, 07/07/2021\n"
     ]
    }
   ],
   "source": [
    "# b) Top 10 women’s ODI Batting players along with the records of their team and ratingimport requests\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "\n",
    "batsmen = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    batsman = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[4].text.strip()\n",
    "    \n",
    "    batsmen.append(batsman)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Batsman': batsmen,'Team': teams,'Rating': ratings})\n",
    "\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494f3816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            All-Rounder Team                          Rating\n",
      "0  Natalie Sciver-Brunt  ENG     421 v Australia, 18/07/2023\n",
      "1      Ashleigh Gardner  AUS       389 v Ireland, 28/07/2023\n",
      "2       Hayley Matthews   WI       392 v Ireland, 26/06/2023\n",
      "3        Marizanne Kapp   SA   419 v West Indies, 10/09/2021\n",
      "4          Ellyse Perry  AUS   548 v West Indies, 11/09/2019\n",
      "5           Amelia Kerr   NZ   356 v West Indies, 25/09/2022\n",
      "6         Deepti Sharma  IND  397 v South Africa, 09/10/2019\n",
      "7         Jess Jonassen  AUS   308 v West Indies, 11/09/2019\n",
      "8         Sophie Devine   NZ     305 v Australia, 05/10/2020\n",
      "9              Nida Dar  PAK     232 v Australia, 21/01/2023\n"
     ]
    }
   ],
   "source": [
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table', class_='table')\n",
    "\n",
    "all_rounders = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    all_rounder = cols[1].text.strip()\n",
    "    team = cols[2].text.strip()\n",
    "    rating = cols[4].text.strip()\n",
    "    \n",
    "    all_rounders.append(all_rounder)\n",
    "    teams.append(team)\n",
    "    ratings.append(rating)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'All-Rounder': all_rounders,'Team': teams,'Rating': ratings})\n",
    "\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "200f5bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper_Title\n",
      "0                                    Reward is enough\n",
      "1   Explanation in artificial intelligence: Insigh...\n",
      "2              Creativity and artificial intelligence\n",
      "3   Conflict-based search for optimal multi-agent ...\n",
      "4   Knowledge graphs as tools for explainable mach...\n",
      "5   Law and logic: A review from an argumentation ...\n",
      "6   Between MDPs and semi-MDPs: A framework for te...\n",
      "7   Explaining individual predictions when feature...\n",
      "8       Multiple object tracking: A literature review\n",
      "9   A survey of inverse reinforcement learning: Ch...\n",
      "10  Evaluating XAI: A comparison of rule-based and...\n",
      "11  Explainable AI tools for legal reasoning about...\n",
      "12            Hard choices in artificial intelligence\n",
      "13  Assessing the communication gap between AI mod...\n",
      "14  Explaining black-box classifiers using post-ho...\n",
      "15  The Hanabi challenge: A new frontier for AI re...\n",
      "16              Wrappers for feature subset selection\n",
      "17  Artificial cognition for social human–robot in...\n",
      "18  A review of possible effects of cognitive bias...\n",
      "19  The multifaceted impact of Ada Lovelace in the...\n",
      "20  Robot ethics: Mapping the issues for a mechani...\n",
      "21          Reward (Mis)design for autonomous driving\n",
      "22  Planning and acting in partially observable st...\n",
      "23  What do we want from Explainable Artificial In...\n"
     ]
    }
   ],
   "source": [
    "# 5.Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\n",
    "\n",
    "#i) Paper Title\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "articles = article = soup.find_all('a', class_=\"sc-5smygv-0 fIXTHm\")\n",
    "                                \n",
    "paper_title = []\n",
    "   \n",
    "for article in articles:\n",
    "    paper_title.append(article.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Paper_Title': paper_title})\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d664e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Authors\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...\n",
      "1                                          Tim Miller\n",
      "2                                   Margaret A. Boden\n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...\n",
      "4                      Ilaria Tiddi, Stefan Schlobach\n",
      "5                      Henry Prakken, Giovanni Sartor\n",
      "6     Richard S. Sutton, Doina Precup, Satinder Singh\n",
      "7           Kjersti Aas, Martin Jullum, Anders Løland\n",
      "8                Wenhan Luo, Junliang Xing and 4 more\n",
      "9                       Saurabh Arora, Prashant Doshi\n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...\n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...\n",
      "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz\n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...\n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...\n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more\n",
      "16                         Ron Kohavi, George H. John\n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more\n",
      "18    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz\n",
      "19                             Luigia Carlucci Aiello\n",
      "20             Patrick Lin, Keith Abney, George Bekey\n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more\n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...\n",
      "23             Markus Langer, Daniel Oster and 6 more\n"
     ]
    }
   ],
   "source": [
    "#ii) Authors\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "articles = article = soup.find_all('span', class_=\"sc-1w3fpd7-0 dnCnAO\")\n",
    "                                \n",
    "Authors = []\n",
    "   \n",
    "for article in articles:\n",
    "    Authors.append(article.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Authors': Authors})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84ae898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Published_Date\n",
      "0     October 2021\n",
      "1    February 2019\n",
      "2      August 1998\n",
      "3    February 2015\n",
      "4     January 2022\n",
      "5     October 2015\n",
      "6      August 1999\n",
      "7   September 2021\n",
      "8       April 2021\n",
      "9      August 2021\n",
      "10   February 2021\n",
      "11      April 2023\n",
      "12   November 2021\n",
      "13      March 2023\n",
      "14        May 2021\n",
      "15      March 2020\n",
      "16   December 1997\n",
      "17       June 2017\n",
      "18       June 2021\n",
      "19       June 2016\n",
      "20      April 2011\n",
      "21      March 2023\n",
      "22        May 1998\n",
      "23       July 2021\n"
     ]
    }
   ],
   "source": [
    "#iii) Published Date\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "articles = article = soup.find_all('span', class_=\"sc-1thf9ly-2 dvggWt\")\n",
    "                                \n",
    "Published_Date = []\n",
    "   \n",
    "for article in articles:\n",
    "    Published_Date.append(article.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Published_Date': Published_Date})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d44919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Paper_URL\n",
      "0   https://www.sciencedirect.com/science/article/...\n",
      "1   https://www.sciencedirect.com/science/article/...\n",
      "2   https://www.sciencedirect.com/science/article/...\n",
      "3   https://www.sciencedirect.com/science/article/...\n",
      "4   https://www.sciencedirect.com/science/article/...\n",
      "5   https://www.sciencedirect.com/science/article/...\n",
      "6   https://www.sciencedirect.com/science/article/...\n",
      "7   https://www.sciencedirect.com/science/article/...\n",
      "8   https://www.sciencedirect.com/science/article/...\n",
      "9   https://www.sciencedirect.com/science/article/...\n",
      "10  https://www.sciencedirect.com/science/article/...\n",
      "11  https://www.sciencedirect.com/science/article/...\n",
      "12  https://www.sciencedirect.com/science/article/...\n",
      "13  https://www.sciencedirect.com/science/article/...\n",
      "14  https://www.sciencedirect.com/science/article/...\n",
      "15  https://www.sciencedirect.com/science/article/...\n",
      "16  https://www.sciencedirect.com/science/article/...\n",
      "17  https://www.sciencedirect.com/science/article/...\n",
      "18  https://www.sciencedirect.com/science/article/...\n",
      "19  https://www.sciencedirect.com/science/article/...\n",
      "20  https://www.sciencedirect.com/science/article/...\n",
      "21  https://www.sciencedirect.com/science/article/...\n",
      "22  https://www.sciencedirect.com/science/article/...\n",
      "23  https://www.sciencedirect.com/science/article/...\n"
     ]
    }
   ],
   "source": [
    "# iv) Paper URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "articles = article = soup.find_all('a', class_=\"sc-5smygv-0 fIXTHm\")\n",
    "                                \n",
    "paper_url = []\n",
    "   \n",
    "for article in articles:\n",
    "    paper_url.append(article['href']) \n",
    "\n",
    "df = pd.DataFrame({'Paper_URL':paper_url})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43fc00c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Restaurant_name\n",
      "0      Castle BarbequeConnaught Place, Central Delhi\n",
      "1  Cafe KnoshThe Leela Ambience Convention Hotel,...\n",
      "2  Castle's BarbequePacific Mall,Tagore Garden, W...\n",
      "3    India GrillHilton Garden Inn,Saket, South Delhi\n",
      "4  The Barbeque CompanyGardens Galleria,Sector 38...\n",
      "5  Delhi BarbequeTaurus Sarovar Portico,Mahipalpu...\n",
      "6  The Monarch - Bar Be Que VillageIndirapuram Ha...\n",
      "7  The Barbeque TimesM2K Corporate Park,Sector 51...\n",
      "8  Indian Grill RoomSuncity Business Tower,Golf C...\n"
     ]
    }
   ],
   "source": [
    "# 7.Write a python program to scrape mentioned details from dineout.co.inand make data frame  i) Restaurant name\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "results = soup.find_all('div', class_=\"restnt-info cursor\")\n",
    "                                \n",
    "restaurant_name = []\n",
    "   \n",
    "for result in results:\n",
    "    restaurant_name.append(result.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Restaurant_name':restaurant_name})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ad7a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Cusine\n",
      "0     ₹ 2,000 for 2 (approx) | Chinese, North Indian\n",
      "1      ₹ 3,000 for 2 (approx) | Italian, Continental\n",
      "2     ₹ 2,000 for 2 (approx) | Chinese, North Indian\n",
      "3     ₹ 2,400 for 2 (approx) | North Indian, Italian\n",
      "4     ₹ 1,700 for 2 (approx) | North Indian, Chinese\n",
      "5              ₹ 1,800 for 2 (approx) | North Indian\n",
      "6              ₹ 1,900 for 2 (approx) | North Indian\n",
      "7  ₹ 1,500 for 2 (approx) | North Indian, Contine...\n",
      "8     ₹ 2,200 for 2 (approx) | North Indian, Mughlai\n"
     ]
    }
   ],
   "source": [
    "##ii) Cuisine\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "results = soup.find_all('div', class_=\"detail-info\")\n",
    "                                \n",
    "cusine = []\n",
    "   \n",
    "for result in results:\n",
    "    cusine.append(result.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Cusine':cusine})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b554f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Location\n",
      "0                     Connaught Place, Central Delhi\n",
      "1  The Leela Ambience Convention Hotel,Shahdara, ...\n",
      "2             Pacific Mall,Tagore Garden, West Delhi\n",
      "3               Hilton Garden Inn,Saket, South Delhi\n",
      "4                 Gardens Galleria,Sector 38A, Noida\n",
      "5     Taurus Sarovar Portico,Mahipalpur, South Delhi\n",
      "6  Indirapuram Habitat Centre,Indirapuram, Ghaziabad\n",
      "7              M2K Corporate Park,Sector 51, Gurgaon\n",
      "8   Suncity Business Tower,Golf Course Road, Gurgaon\n"
     ]
    }
   ],
   "source": [
    "#iii) Location\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "results = soup.find_all('div', class_=\"restnt-loc ellipsis\")\n",
    "                                \n",
    "location = []\n",
    "   \n",
    "for result in results:\n",
    "    location.append(result.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Location': location})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f60dd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ratings\n",
      "0       4\n",
      "1     4.3\n",
      "2     3.9\n",
      "3     3.9\n",
      "4     3.9\n",
      "5     3.7\n",
      "6     3.8\n",
      "7     4.1\n",
      "8     4.3\n"
     ]
    }
   ],
   "source": [
    "#iv) Ratings\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "results = soup.find_all('div', class_=\"restnt-rating rating-4\")\n",
    "                                \n",
    "ratings = []\n",
    "   \n",
    "for result in results:\n",
    "    ratings.append(result.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Ratings':ratings})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd11e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Images\n",
      "0  https://im1.dineout.co.in/images/uploads/resta...\n",
      "1  https://im1.dineout.co.in/images/uploads/resta...\n",
      "2  https://im1.dineout.co.in/images/uploads/resta...\n",
      "3  https://im1.dineout.co.in/images/uploads/resta...\n",
      "4  https://im1.dineout.co.in/images/uploads/resta...\n",
      "5  https://im1.dineout.co.in/images/uploads/resta...\n",
      "6  https://im1.dineout.co.in/images/uploads/resta...\n",
      "7  https://im1.dineout.co.in/images/uploads/resta...\n",
      "8  https://im1.dineout.co.in/images/uploads/resta...\n"
     ]
    }
   ],
   "source": [
    "# ) Image URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "results = soup.find_all(\"img\", class_=\"no-img\")\n",
    "                                \n",
    "images = []\n",
    "   \n",
    "for result in results:\n",
    "    images.append(result['data-src']) \n",
    "\n",
    "df = pd.DataFrame({'Images':images})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0eb5572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline\n",
      "0   Why 'career choices' is the No. 1 conflict amo...\n",
      "1   Cruise will reduce robotaxi fleet by 50% in Sa...\n",
      "2   Can American-made weapons like F-16s turn the ...\n",
      "3   Harvard gut doctor avoids these 4 foods that c...\n",
      "4   The clash of sustainability and AI is creating...\n",
      "5   On tap next week: 2 housing reports and 2 Inve...\n",
      "6          Top 10 best European cities for retirement\n",
      "7   Google's plan to purge inactive accounts isn't...\n",
      "8   The No. 1 best state to retire in the U.S.—it'...\n",
      "9   Mark Cuban passed on an Uber investment that c...\n",
      "10  Believing these 5 Social Security myths may re...\n",
      "11  Here's why Aldi is looking to the Southern U.S...\n",
      "12  Here's how the Huy Fong Foods sriracha shortag...\n",
      "13  This ETF is soaring in August as market swoon ...\n",
      "14  Morgan Stanley is among the most oversold in t...\n",
      "15  Buy these stocks with upside as market fears i...\n",
      "16  Palo Alto shares rise on earnings beat, after ...\n",
      "17  Wall Street awaits hotly anticipated Nvidia ea...\n",
      "18  WeWork plunges another 11% after announcing re...\n",
      "19  The iPhone 15 could get one of the biggest upg...\n",
      "20  Coral bleaching event in Florida is 'just the ...\n",
      "21  Bitcoin is giving a bearish signal. Here’s wha...\n",
      "22  Earnings show shoppers will spend money for va...\n",
      "23  Nvidia, key Powell speech to take center stage...\n",
      "24        My HomePod is now a very expensive doorstop\n",
      "25  3 trends are dividing restaurant companies int...\n",
      "26  How hurricanes may affect the 2024 Social Secu...\n",
      "27  Rosenblatt names its top picks to play the ‘ag...\n",
      "28  It may be tough for Apple to outperform from h...\n",
      "29  'Blue Beetle' tries to take down 'Barbie' in a...\n"
     ]
    }
   ],
   "source": [
    "#6. Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame \n",
    "#i) Headline\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all('a', class_=\"LatestNews-headline\")\n",
    "                                \n",
    "Headline = []\n",
    "   \n",
    "for article in articles:\n",
    "    Headline.append(article.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Headline': Headline})\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63eba69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Time\n",
      "0       2 Hours Ago\n",
      "1       3 Hours Ago\n",
      "2       4 Hours Ago\n",
      "3       6 Hours Ago\n",
      "4       6 Hours Ago\n",
      "5       6 Hours Ago\n",
      "6       6 Hours Ago\n",
      "7       7 Hours Ago\n",
      "8       7 Hours Ago\n",
      "9       7 Hours Ago\n",
      "10      7 Hours Ago\n",
      "11      8 Hours Ago\n",
      "12      8 Hours Ago\n",
      "13      8 Hours Ago\n",
      "14      8 Hours Ago\n",
      "15      8 Hours Ago\n",
      "16     23 Hours Ago\n",
      "17     23 Hours Ago\n",
      "18     23 Hours Ago\n",
      "19     24 Hours Ago\n",
      "20  August 18, 2023\n",
      "21  August 18, 2023\n",
      "22  August 18, 2023\n",
      "23  August 18, 2023\n",
      "24  August 18, 2023\n",
      "25  August 18, 2023\n",
      "26  August 18, 2023\n",
      "27  August 18, 2023\n",
      "28  August 18, 2023\n",
      "29  August 18, 2023\n"
     ]
    }
   ],
   "source": [
    "#ii) Time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all('span', class_=\"LatestNews-wrapper\")\n",
    "                                \n",
    "time = []\n",
    "   \n",
    "for article in articles:\n",
    "    time.append(article.text.strip()) \n",
    "\n",
    "df = pd.DataFrame({'Time': time})\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43e8a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Link\n",
      "0   https://www.cnbc.com/2023/08/19/why-career-cho...\n",
      "1   https://www.cnbc.com/2023/08/19/cruise-will-re...\n",
      "2   https://www.cnbc.com/2023/08/19/can-expensive-...\n",
      "3   https://www.cnbc.com/2023/08/19/harvard-gut-do...\n",
      "4   https://www.cnbc.com/2023/08/19/the-clash-of-s...\n",
      "5   https://www.cnbc.com/2023/08/19/on-tap-next-we...\n",
      "6   https://www.cnbc.com/2023/08/19/best-countries...\n",
      "7   https://www.cnbc.com/2023/08/19/google-faces-c...\n",
      "8   https://www.cnbc.com/2023/08/19/best-us-states...\n",
      "9   https://www.cnbc.com/2023/08/19/mark-cuban-pas...\n",
      "10  https://www.cnbc.com/2023/08/19/social-securit...\n",
      "11  https://www.cnbc.com/2023/08/19/aldi-winn-dixi...\n",
      "12  https://www.cnbc.com/2023/08/19/how-did-the-hu...\n",
      "13  https://www.cnbc.com/2023/08/19/this-etf-is-so...\n",
      "14  https://www.cnbc.com/2023/08/19/morgan-stanley...\n",
      "15  https://www.cnbc.com/2023/08/19/goldman-picks-...\n",
      "16  https://www.cnbc.com/2023/08/18/palo-alto-netw...\n",
      "17  https://www.cnbc.com/2023/08/18/wall-street-pr...\n",
      "18  https://www.cnbc.com/2023/08/18/wework-plunges...\n",
      "19  https://www.cnbc.com/2023/08/18/iphone-15-usb-...\n",
      "20  https://www.cnbc.com/2023/08/18/noaa-florida-c...\n",
      "21  https://www.cnbc.com/2023/08/18/bitcoin-is-giv...\n",
      "22  https://www.cnbc.com/2023/08/18/retail-earning...\n",
      "23  https://www.cnbc.com/2023/08/18/nvidia-powell-...\n",
      "24  https://www.cnbc.com/2023/08/18/apple-wont-rep...\n",
      "25  https://www.cnbc.com/2023/08/18/mcdonalds-chip...\n",
      "26  https://www.cnbc.com/2023/08/18/social-securit...\n",
      "27  https://www.cnbc.com/2023/08/18/rosenblatt-nam...\n",
      "28  https://www.cnbc.com/2023/08/18/it-may-be-toug...\n",
      "29  https://www.cnbc.com/2023/08/18/barbie-vs-blue...\n"
     ]
    }
   ],
   "source": [
    "# iii) News Link\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all('a', class_=\"LatestNews-headline\")\n",
    "                                \n",
    "link = []\n",
    "   \n",
    "for article in articles:\n",
    "    link.append(article['href']) \n",
    "\n",
    "df = pd.DataFrame({'Link':link})\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
